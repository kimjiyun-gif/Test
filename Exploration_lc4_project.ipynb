{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hired-northern",
   "metadata": {},
   "source": [
    "# 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deluxe-oxford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free', 'They come to join us here', 'From sea to shining sea And they all have a dream', 'As people always will', 'To be safe and warm', 'In that shining city on the hill Some wanna slam the door', 'Instead of opening the gate', \"Aw, let's turn this thing around\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-effort",
   "metadata": {},
   "source": [
    "# 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "flying-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developed-student",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "popular-replacement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175749,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> verse <end>',\n",
       " '<start> they come from everywhere <end>',\n",
       " '<start> a longing to be free <end>',\n",
       " '<start> they come to join us here <end>',\n",
       " '<start> from sea to shining sea and they all have a dream <end>',\n",
       " '<start> as people always will <end>',\n",
       " '<start> to be safe and warm <end>',\n",
       " '<start> in that shining city on the hill some wanna slam the door <end>',\n",
       " '<start> instead of opening the gate <end>',\n",
       " '<start> aw , let s turn this thing around <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "print(np.array(corpus).shape)\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unlike-newcastle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  520    3 ...    0    0    0]\n",
      " [   2   45   66 ...    0    0    0]\n",
      " [   2    9 3390 ...    0    0    0]\n",
      " ...\n",
      " [   2  561   21 ...    0    0    0]\n",
      " [   2  120   34 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f0381b9e7d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor_tmp = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor=[]\n",
    "    for i in tensor_tmp:\n",
    "        if len(i) > 15: \n",
    "            continue\n",
    "        else:\n",
    "            tensor.append(i)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bearing-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proprietary-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  520    3    0    0    0    0    0    0    0]\n",
      " [   2   45   66   74  808    3    0    0    0    0]\n",
      " [   2    9 3390   10   27  270    3    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efficient-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 520   3   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[520   3   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-latter",
   "metadata": {},
   "source": [
    "# 평가 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "japanese-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터셋 만들기\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "considerable-riverside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_train\n",
    "\n",
    "BUFFER_SIZE = len(enc_val)\n",
    "steps_per_epoch = len(enc_val) // BATCH_SIZE\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-kitchen",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "identical-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "invalid-initial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 14)\n"
     ]
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset_train.take(1): break\n",
    "\n",
    "model(src_sample)\n",
    "print(src_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "willing-rescue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "received-cologne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 165s 334ms/step - loss: 4.0163 - val_loss: 3.1445\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 164s 336ms/step - loss: 3.0626 - val_loss: 2.9596\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 164s 337ms/step - loss: 2.8836 - val_loss: 2.8527\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 164s 336ms/step - loss: 2.7551 - val_loss: 2.7737\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 163s 334ms/step - loss: 2.6497 - val_loss: 2.7143\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 163s 334ms/step - loss: 2.5623 - val_loss: 2.6652\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 165s 338ms/step - loss: 2.4645 - val_loss: 2.6242\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 164s 338ms/step - loss: 2.3880 - val_loss: 2.5888\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 164s 336ms/step - loss: 2.3089 - val_loss: 2.5587\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 163s 335ms/step - loss: 2.2410 - val_loss: 2.5346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f037f7f8950>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 과정에서 epoch 시간이 너무 오래걸려 30회에서 10회로 줄였다.\n",
    "# 밑의 결과를 보면 알 수 있듯이 10회의 epoch으로 loss가 2.2410으로 줄어들었다.\n",
    "# 30회의 epoch이면 loss의 값이 2.2 이하로 내려갔을 것임을 알 수 있다.\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset_train, epochs=10, validation_data = dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-leisure",
   "metadata": {},
   "source": [
    "# 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "varying-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interesting-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerical-toyota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-wells",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-croatia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-alpha",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-crowd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-narrative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-linux",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
